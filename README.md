# Open Swarm

<div align="center">
<img src="assets/images/openswarm-project-image.jpg" alt="Project Logo" width="70%"/>
</div>

**Open Swarm** is a versatile, modular framework for building intelligent, multi-agent systems. It's a **fork and actively maintained extension** of the [OpenAI Swarm](https://github.com/openai/swarm) framework. It includes modifications to support stateless RESTful operations and a plugin system for custom extensions that enhance agentic workflows.

---

https://github.com/user-attachments/assets/1335f7fb-ff61-4e96-881c-7d3154eb9f14

(generated by www.gitpodcast.com)

---

## Table of Contents
- [Key Features](#key-features)
- [Blueprints](#blueprints)
- [Operational Modes](#operational-modes)
- [Configuration & Multiple LLM Providers](#configuration--multiple-llm-providers)
- [Installation](#installation)
- [Running Open Swarm](#running-open-swarm)
- [Deploying with Docker](#deploying-with-docker)
  - [Deploy with Docker Compose (Recommended)](#deploy-with-docker-compose-recommended)
  - [Deploy Standalone](#deploy-standalone)
- [Progress Tracker](#progress-tracker)
- [Further Documentation](#further-documentation)
- [License](#license)
- [Acknowledgements](#acknowledgements)

---

## Key Features

1. **Multi-Agent Orchestration**  
   - Define multiple agents, each with unique instructions and roles.
   - Agents coordinate tasks, share context, or hand off queries between one another.

2. **Blueprint-Driven Architecture**  
   - Each **Blueprint** encapsulates logic, tool connections, and environment/config settings.
   - Encourages reusable, modular patterns for different use cases.

3. **Optional MCP or GPT Actions**  
   - (WIP) Integrate with external tools (e.g., databases, web search, filesystems) through **MCP servers**.
   - (TODO) Use **GPT Actions** as an alternative for agent expansions without dedicated MCP infrastructure.

4. **CLI & REST Interface**  
   - (WIP) A setup wizard helps define or update blueprint configurations.
   - Run from the command line or expose a Django-powered REST API for broader integration.
   - Interactive web pages per blueprint at `/<blueprint_name>/`.

5. **OpenAI API Compatibility**  
   - Exposes an endpoint at `/v1/chat/completions` that is functionally similar to the OpenAI Chat Completions API.
   - Includes a **mandatory** `sender` field in agent responses.  
     - This field identifies which Swarm agent provided the response and must be preserved in the conversation history for proper handoffs between agents.
     - While the framework is compatible with OpenAI-like API clients, it assumes the client application maintains the `sender` field and, ideally, displays it in the user interface.
     - **Note:** Most OpenAI API-compatible applications will ignore the `sender` field by default and not display the agent name. Custom UI or logic is required to utilise and present this information.

6. **(WIP) Configurable LLM Providers**  
   - Supports multiple OpenAI-compatible providers in a single environment (e.g., `openai`, `grok`, `ollama`).
   - Allows specifying different models/providers for different agents—even within the same blueprint.
   - Use environment variable `DEFAULT_LLM` to specify default llm model provider used by blueprints, ie `DEFAULT_LLM=ollama`

---

## Blueprints

A **Blueprint** is a Python module that wraps:

- **Agent Logic**: Defines how each agent in the Swarm processes user messages, whether it calls tools, and how it decides to hand off to other agents.
- **Tools**: Specifies which agents have which tools (e.g., MCP-discovered tools, Python function calls).
- **Environment & Configuration**: Ensures required environment variables and JSON configs are validated prior to agent execution.

Once registered, a blueprint is discoverable at runtime, allowing the system to list and load agents on demand.

### Personal Assistant Example

The **Personal Assistant Blueprint** demonstrates a hybrid approach, integrating **local Python function tools** with **MCP-discovered tools**. It consists of:

1. **Personal Assistant Agent**  
   - Determines user intent and delegates queries accordingly.  
   - Routes weather-related queries to the `WeatherAgent`.  
   - Routes knowledge-based queries to the `DocumentationAgent`.  

2. **Weather Agent** (Uses Python Function Tools)  
   - Fetches current weather and forecasts via OpenWeatherMap.  
   - Uses a **locally defined Python function** rather than an MCP server.  
   - Requires `WEATHER_API_KEY` as an environment variable.

3. **Documentation Agent** (Uses MCP-Discovered Tools)  
   - Retrieves relevant documentation via `rag-docs`.  
   - Uses the MCP function `search_documentation` to dynamically retrieve information.  
   - Requires the following environment variables:  
     - `OPENAI_API_KEY`  
     - `QDRANT_URL`  
     - `QDRANT_API_KEY`

This blueprint highlights **seamless multi-agent coordination** and the **flexibility of combining Python functions with MCP-discovered tools**.

### Other Examples

Open Swarm includes a growing library of **Blueprint** examples:

| Blueprint Name               | Description                                                                 |
|------------------------------|-----------------------------------------------------------------------------|
| **Echo Blueprint**           | A straightforward agent that simply echoes user inputs—ideal for testing or as a starter template. |
| **Personal Assistant Blueprint** | Combines real-time weather updates (Python function) with documentation search (`rag-docs`, MCP). Demonstrates mixed tooling. |
| **Database and Web Blueprint** | Demonstrates MCP-based integration with an SQLite database and Brave Search, illustrating how to combine data retrieval with real-time web queries. |
| **Sysadmin Blueprint**        | Multi-agent system for handling system administration tasks using MCP tools (filesystem, SQLite, search, etc.). |

---

## Operational Modes

1. **REST Mode**  
   - Launch Django with `uv run manage.py runserver 0.0.0.0:8000`.  
   - Access endpoints:
     - `POST /v1/chat/completions`: Chat-style agent interactions (OpenAI-compatible).
     - `GET /v1/models`: Lists available blueprints.
     - `http://localhost:8000/<blueprint_name>/`: Interactive, web-based blueprint tester.
   - (TODO) Optionally integrate with Django Admin at `/admin`.

2. **CLI Mode**  
   - Execute specific blueprint files (e.g., `uv run blueprints/university/blueprint_university.py`).  
   - Great for local testing, debugging, and iterative development.
  
![image](https://github.com/user-attachments/assets/8593c382-9f2d-4d7a-ba41-0330449b3f38)

---

## Configuration & Multiple LLM Providers

Open Swarm uses:
- **`.env`** files for API keys or critical environment variables (e.g., `OPENAI_API_KEY`).  
- **`swarm_config.json`** (or custom JSON) for advanced settings:
  - **`llm`**: Define multiple OpenAI-compatible endpoints (e.g., `openai`, `grok`, `ollama`).
  - **`mcp_servers`**: Tools/services that agents can call.
  - **`gpt_actions`**: (TODO) More tools/services that agents can call.

### Note:
If you want to bypass API key validation for a provider (e.g., `ollama`), set the `api_key` field to an empty string (`""`). This can be useful for providers or testing configurations that do not require an API key.

Different agents in a single blueprint can reference different LLM providers. For example:
```json
{
  "llm": {
    "openai": {
      "provider": "openai",
      "model": "gpt-4",
      "base_url": "https://api.openai.com/v1",
      "api_key": "${OPENAI_API_KEY}"
    },
    "grok": {
      "provider": "openai",
      "model": "grok-2-1212",
      "base_url": "https://api.x.ai/v1",
      "api_key": "${XAI_API_KEY}"
    },
    "ollama": {
      "provider": "openai",
      "model": "llama3.2",
      "base_url": "http://localhost:11434/v1",
      "api_key": ""
    }
  }
}
```
These references let you quickly switch providers based on environment or agent specificity.

---

## Installation

1. **Clone the Repository**  
   ```bash
   git clone https://github.com/matthewhand/open-swarm.git
   cd open-swarm
   ```
2. **Install Dependencies**  
   ```bash
   # Install 'uv' => https://docs.astral.sh/uv/
   uv python install 3.12
   uv venv
   source .venv/bin/activate
   uv sync
   ```
3. **Environment Setup**  
   - Copy `.env.example` to `.env` and fill in sensitive details (`OPENAI_API_KEY`, etc.).
   ```bash
   cp .env.example .env
   vi .env
   ```
   - *(Optional)* Update `swarm_config.json` to add or modify LLM providers, MCP servers, etc.

---

## Running Open Swarm

### Running with the REST API

1.  **Start the Django REST API Server:**
    ```bash
    uv run manage.py migrate
    uv run manage.py runserver 0.0.0.0:8000
    ```

2.  **Access the Interactive Blueprint Pages:**
    *   Open your web browser and visit:
        *   `http://localhost:8000/<blueprint_name>` (e.g., `http://localhost:8000/university`)

        *   You will see a text input where you can type queries.
        *   The `sender` of the response (the name of the agent that responded) will be shown above each response.
        *   Below is a screenshot showing an example of the interactive HTML page:

            <img src="assets/images/20250105-Open-Swarm-HTML-Page.png" alt="Interactive Chat Interface" width="70%"/>

3.  **Integrate with Open WebUI:**
    *    Open Swarm has full compatibility with OpenAI API-compatible UIs, such as [Open WebUI](https://github.com/open-webui/open-webui). By using a client like Open WebUI you will not only see the `sender` field, but also experience a more engaging chat UI with other features.
    *   To configure Open WebUI to use Open Swarm:
        *   Start the REST API server via `uv run manage.py runserver 0.0.0.0:8000`
        *   Install the custom function from the [Open WebUI Functions Hub](https://openwebui.com/f/matthewh/swarm_manifold).
        *   In the custom function valve settings, change the API Base URL if different to the default, `http://host.docker.internal:8000`

    * To see a demo of Open WebUI with the University Blueprint with expressive voice output, please see the following demonstration video:

https://github.com/user-attachments/assets/a4688100-5737-479f-91e5-974db98296d7

5.  **Access the REST Endpoints Directly:**
   You can also interact with the API using a tool like `curl`. For example:
    ```bash
    curl -X POST http://localhost:8000/v1/chat/completions \
        -H "Content-Type: application/json" \
        -d '{"model":"university","messages":[{"role":"user","content":"What courses should I take next semester if I’m interested in data science?"}]}'
    ```
    *   You will see a JSON response, containing the `sender` field within the response (in `data.choices[0].message.sender`).

---

### Running with the CLI

1.  **Execute a Specific Blueprint:**
    ```bash
    uv run blueprints/university/blueprint_university.py
    ```
    *   This will execute the `UniversitySupportBlueprint` in interactive mode, starting with the `TriageAgent`.
    *   You can then enter a query at the prompt, and the system will automatically route this through the appropriate agent, with any handoffs between agents done automatically by the Swarm framework.
    *  Try entering queries like:
        *   `"What courses should I take next semester if I’m interested in data science?"` to test the handoff to the `CourseAdvisor`
        *   `"Write me a poem about the university cafeteria,"` to test the handoff to the `UniversityPoet`
        *  `"What time is the Artificial Intelligence exam?"` to test the handoff to the `SchedulingAssistant`
     * Please note that using the CLI will only show the default text responses. To experience the multi-agent orchestration with a full UI, and with voice, it is recommended to use Open WebUI (see the REST instructions below)

## Deploying with Docker

### Deploy with Docker Compose (Recommended)

1. **Obtain `docker-compose.yaml`**  
   ```bash
   wget https://raw.githubusercontent.com/matthewhand/open-swarm/refs/heads/main/docker-compose.yaml
   ```

2. **Set up `.env`**  
   Retrieve the `.env` template and configure the `OPENAI_API_KEY`:
   ```bash
   wget https://raw.githubusercontent.com/matthewhand/open-swarm/refs/heads/main/.env.example -O .env
   sed -i 's/^OPENAI_API_KEY=.*/OPENAI_API_KEY=your_openai_api_key_here/' .env
   ```
   Replace `your_openai_api_key_here` with your actual OpenAI API key.

3. **(Optional) Adjust `swarm_config.json`**  
   Download and modify `swarm_config.json` if you plan to use local LLM endpoints or different providers.

4. **Start the Service**  
   ```bash
   docker compose up -d
   ```
   This:
   - Builds the image if needed.
   - Reads port settings and environment variables from `.env`.
   - Exposes the application on `8000` (unless overridden via `$PORT`).

5. **Access the Application**  
   - Visit [http://localhost:8000](http://localhost:8000) for the interactive blueprint pages.

### Deploy Standalone 

1. Configure `.env` (mandatory) and `swarm_config.json` (optional) as above

2. Run following cmd:
```bash
docker run \
  --env-file .env \
  -p ${PORT:-8000}:${PORT:-8000} \
  -v ./blueprints:/app/blueprints \
  -v ./swarm_config.json:/app/src/swarm/swarm_config.json \
  --name open-swarm \
  --restart unless-stopped \
  mhand79/open-swarm:latest
```

---

## Diagram: Backend HTTP Service Overview

Below is a simplified diagram illustrating how the **Open Swarm** HTTP service can function as a backend for any OpenAI API-compatible client or tool. The service lists configured **Blueprints** via `/v1/models` and performs inference through the `/v1/chat/completions` endpoint. Internally, it can call out to any configured **OpenAI-compatible LLM provider** (OpenAI, Grok, Ollama, etc.) and optionally run **GPT Actions** or **MCP servers** (like database, filesystem, or weather integrations).

```
 ┌─────────────────────────────────────────────────────────────────────┐
 │        OpenAI-Compatible Client Tools that displays sender        │
 │                      e.g. Open-WebUI                              │
 └────────────┬───────────────────────────────────────────────────────┘
              |                             
              |   (HTTP: /v1/chat/completions, /v1/models)
              ▼                             
 ┌─────────────────────────────────────────────────────────────────────┐
 │                 Open Swarm REST API Service (Django)              │
 │  (Exposes /v1/models, /v1/chat/completions, /admin, /<blueprint>  │
 └─────────────────────────────────────────────────────────────────────┘
                     |                        | 
                     |                        | MCP Servers and 
                     |                        | GPT Actions (WIP) 
       LLM Inference |                        | (filesystem,    
                     |                        | database, etc.)           
                     ▼                        ▼                
       ┌────────────────────────┐         ┌────────────────────────┐
       │OpenAI-Compatible LLMs  │         │ External APIs/Services │
       │ (OpenAI, Grok, Ollama) │         │ (Weather, Database, ..)│
       └────────────────────────┘         └────────────────────────┘
```

---

## Progress Tracker

- **REST Mode**  
  - [x] Inference via `/v1/chat/completions`  
  - [x] Blueprints listed via `/v1/models/`  
  - [x] Execute blueprints via `/<blueprint>` i.e. http://localhost:8000/university
  - [x] Simple HTML page
  - [ ] Application management via `/admin`  
   - [x] User management
   - [ ] Blueprint management
  - [ ] Streaming chat (django_chat)

- **CLI Mode**  
  - [ ] Setup Wizard  
  - [x] Blueprint Runner  

- **Multiple LLM Providers**  
  - [ ] Assign different models per agent in one blueprint  

- **Tooling Integration Frameworks**  
  - [x] MCP Servers
  - [x] Use official mcp python sdk 
  - [ ] GPT Actions
  - [ ] Load claude desktop mcp server config
  - [ ] Load roo-code mcp server config

- **Deployment**  
  - [x] Dockerfile and docker-compose.yaml  
  - [x] Publish to Docker Registry  
  - [ ] Publish Python module to PyPI  

- **Example Blueprints**  
  - [x] `university`  
  - [x] `echo`
  - [x] `weather`  
  - [x] `filesystem`  
  - [x] `database_and_web` (SQLite & Brave Search)  
  - [ ] `flowise` (Requires Flowise server or cloud account)

- **Security**  
  - [ ] Securing REST completions endpoint with API_KEY
  - [ ] CORS access control

---

## Further Documentation

For advanced usage, sequence diagrams, or in-depth tooling examples, see [DEVELOPMENT.md](./DEVELOPMENT.md). Additional expansions and best practices for agent orchestration, LLM provider swapping, and more can be found in that document.

---

## License

Open Swarm is provided under the MIT License. Refer to the [LICENSE](LICENSE) file for full details.

## Acknowledgements

This project is based on the [OpenAI Swarm](https://github.com/openai/swarm) framework. We would like to acknowledge the original authors and contributors of this project for their work.  
We also wish to credit [django_chatbot](https://github.com/MattiPaivike/django_chatbot) for the Django chatbot view.

### Third-Party Libraries
- **[Marked.js](https://github.com/markedjs/marked)** (MIT License)  
  A fast, lightweight library for parsing Markdown into HTML.
- **[Font Awesome](https://github.com/FortAwesome/Font-Awesome)** (CC BY 4.0 License)  
  A popular icon library for web projects. Included for styling buttons and enhancing the user interface.
